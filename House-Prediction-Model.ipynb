{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning model to predict the prices of home in IOWA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the .csv file into dataset\n",
    "def load_data(filename):\n",
    "    dataset = pd.read_csv(filename)\n",
    "    return dataset\n",
    "\n",
    "#splitting the data into target and feature variables\n",
    "def feature_target_data(dataset, feature):\n",
    "    target = dataset[feature]\n",
    "    feature = dataset[dataset.columns.tolist()[:-1]]\n",
    "    return feature, target\n",
    "\n",
    "#finding if any features are almost null in all the examples\n",
    "def finding_null_variables(dataset):\n",
    "    null_variables = []\n",
    "    for i in dataset.columns.tolist():\n",
    "        if dataset[i].isnull().sum() > 700 :\n",
    "            null_variables.append(i)\n",
    "    return null_variables\n",
    "\n",
    "#handling the null values\n",
    "def replacing_the_null_values(dataset, numerical_cols, categorical_cols):\n",
    "    for i in numerical_cols:\n",
    "        dataset.loc[dataset[i].isnull() , i] = 0\n",
    "    for i in categorical_cols:\n",
    "        dataset.loc[dataset[i].isnull(), i] = 'Not-Available'\n",
    "    return dataset\n",
    "\n",
    "#remove some irrelavant features from dataset\n",
    "def update_dataset(dataset, features_to_remove):\n",
    "    new_features = set(dataset.columns.tolist()) - set(features_to_remove)\n",
    "    updated_dataset = dataset[list(new_features)]\n",
    "    return updated_dataset\n",
    "\n",
    "#categorizing numerical and categorical features\n",
    "def split_categorical_numerical_variables(dataset):\n",
    "    numerical_cols = dataset.describe(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = list(set(dataset.columns.tolist()) - set(numerical_cols) - {'Street'})\n",
    "    numerical_cols = list(set(numerical_cols) - {'Id', 'SalePrice'})\n",
    "    return numerical_cols, categorical_cols\n",
    "\n",
    "#merge train and test data if we can see the difference in unique category of both the datasets\n",
    "def consolidate(train, test):\n",
    "    train_test = pd.concat([train, test])\n",
    "    return train_test\n",
    "\n",
    "#drop some feature from dataset\n",
    "def drop_feature(dataset, feature):\n",
    "    updated_dataset = dataset.drop(columns = feature)\n",
    "    return updated_dataset\n",
    "\n",
    "#one_hot_encoding of the categorical varaiables\n",
    "def one_hot_encoding(dataset, categorical_cols):\n",
    "    keys = range(len(dataset.columns.tolist()))\n",
    "    keys_categorical_value = dict(zip(keys, dataset.columns.tolist() ))\n",
    "    labelencoder = LabelEncoder()\n",
    "    categorical_keys = []\n",
    "    dataset_for_training = dataset.iloc[:,:].values\n",
    "    for i in keys_categorical_value.keys():\n",
    "        if keys_categorical_value[i] in categorical_cols:\n",
    "            dataset_for_training[:,i] = labelencoder.fit_transform(dataset_for_training[:, i].astype(str))   \n",
    "            categorical_keys.append(i)\n",
    "    onehotencoder = OneHotEncoder(categorical_features=categorical_keys)\n",
    "    dataset_for_training = onehotencoder.fit_transform(dataset_for_training).toarray()\n",
    "    return dataset_for_training\n",
    "\n",
    "#splitting the train and test datasets\n",
    "def train_test_split(dataset):\n",
    "    training_data = dataset[:1460]\n",
    "    test_data = dataset[1460:]\n",
    "    return training_data, test_data\n",
    "\n",
    "#model training\n",
    "def train_model(model, feature_df, target_df, num_procs, mean_mse, cv_std):\n",
    "    neg_mse = cross_val_score(model, feature_df, target_df, cv=5, n_jobs=num_procs, scoring='neg_mean_squared_error')\n",
    "    mean_mse = -1.0*np.mean(neg_mse)\n",
    "    cv_std = np.std(neg_mse)\n",
    "    print('\\nModel:\\n', model)\n",
    "    print('Average MSE:\\n', mean_mse)\n",
    "    print('Standard deviation during CV:\\n', cv_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data('train.csv')\n",
    "testing = load_data('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, target = feature_target_data(dataset, 'SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Null_Features******\n",
      "['Alley', 'PoolQC', 'Fence', 'MiscFeature']\n",
      "\n",
      "******Numerical_Columns******\n",
      "['LowQualFinSF', 'TotRmsAbvGrd', 'WoodDeckSF', 'MSSubClass', 'LotArea', 'BsmtFullBath', '3SsnPorch', 'MiscVal', 'GarageArea', 'YrSold', 'BsmtHalfBath', 'LotFrontage', 'KitchenAbvGr', 'PoolArea', 'Fireplaces', 'YearRemodAdd', 'HalfBath', 'GarageCars', 'OpenPorchSF', '1stFlrSF', 'EnclosedPorch', 'BedroomAbvGr', 'ScreenPorch', 'GarageYrBlt', 'GrLivArea', 'FullBath', '2ndFlrSF', 'BsmtFinSF1', 'OverallQual', 'TotalBsmtSF', 'YearBuilt', 'MoSold', 'MasVnrArea', 'BsmtFinSF2', 'BsmtUnfSF', 'OverallCond']\n",
      "\n",
      "******Categorical_Columns******\n",
      "['BsmtFinType1', 'HouseStyle', 'RoofMatl', 'Utilities', 'BsmtExposure', 'CentralAir', 'Condition1', 'Exterior1st', 'LotConfig', 'Neighborhood', 'Heating', 'BsmtQual', 'ExterQual', 'FireplaceQu', 'MSZoning', 'SaleType', 'MasVnrType', 'LandSlope', 'Condition2', 'BsmtCond', 'Exterior2nd', 'Electrical', 'BsmtFinType2', 'SaleCondition', 'Foundation', 'GarageType', 'BldgType', 'GarageQual', 'PavedDrive', 'GarageFinish', 'HeatingQC', 'KitchenQual', 'LotShape', 'ExterCond', 'RoofStyle', 'Functional', 'LandContour', 'GarageCond']\n"
     ]
    }
   ],
   "source": [
    "null_features = finding_null_variables(training) #features which are almost null in the dataset\n",
    "print('******Null_Features******')\n",
    "print(null_features)\n",
    "print()\n",
    "training = update_dataset(training, null_features) #update the training set\n",
    "testing = update_dataset(testing, null_features) #update the testing set\n",
    "\n",
    "#finding the numerical and categorical features\n",
    "numerical_cols, categorical_cols = split_categorical_numerical_variables(training)\n",
    "\n",
    "print('******Numerical_Columns******')\n",
    "print(numerical_cols)\n",
    "print()\n",
    "print('******Categorical_Columns******')\n",
    "print(categorical_cols)\n",
    "\n",
    "#consolidating the train and test as unique features are missing in test dataset\n",
    "train_test = consolidate(training, testing)\n",
    "\n",
    "#handling the null_values or empty values from the consolidated dataset\n",
    "train_test = replacing_the_null_values(train_test, numerical_cols, categorical_cols)\n",
    "\n",
    "#drop some features which are not in use\n",
    "train_test = drop_feature(train_test, 'Street')\n",
    "train_test = drop_feature(train_test, 'Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data like label encoding and onehotencoding to create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_for_training = one_hot_encoding(train_test, categorical_cols) #one hot encoding \n",
    "training_data, test_data = train_test_split(dataset_for_training) #splitting the data back to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation among Gradient Boosting and Random Forest Regression using K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning cross validation\n",
      "\n",
      "Model:\n",
      " RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
      "           max_features=None, max_leaf_nodes=None,\n",
      "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "           min_samples_leaf=1, min_samples_split=60,\n",
      "           min_weight_fraction_leaf=0.0, n_estimators=150, n_jobs=2,\n",
      "           oob_score=False, random_state=None, verbose=0, warm_start=False)\n",
      "Average MSE:\n",
      " 1217272019.91\n",
      "Standard deviation during CV:\n",
      " 322877766.807\n",
      "\n",
      "Model:\n",
      " GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.1, loss='ls', max_depth=25, max_features=None,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=60, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=150, presort='auto', random_state=None,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "Average MSE:\n",
      " 716665279.068\n",
      "Standard deviation during CV:\n",
      " 200953268.733\n"
     ]
    }
   ],
   "source": [
    "num_procs = 2 #number of process in parallel\n",
    "\n",
    "#initialize some neccessary dicts and lists\n",
    "models = []\n",
    "mean_mse = {}\n",
    "cv_std = {}\n",
    "res = {}\n",
    "verbose_lvl = 0\n",
    "rf = RandomForestRegressor(n_estimators=150, n_jobs=num_procs, max_depth=25, min_samples_split=60, max_features=None)\n",
    "gbm = GradientBoostingRegressor(n_estimators=150, max_depth=25, loss='ls', max_features=None, min_samples_split=60)\n",
    "models.extend([rf, gbm])\n",
    "#parallel cross-validate models, using MSE as evaluation metric, and print summaries\n",
    "print(\"Beginning cross validation\")\n",
    "for model in models:\n",
    "    train_model(model, training_data, target, num_procs, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### According to results Gradient Boosting got better results so will proceed with Gradient Boosting and tune it using Grid Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'learning_rate': [0.16, 0.18, 0.185], 'n_estimators': [150, 175, 200], 'min_samples_split' : [5,10,15,20],\n",
    "               'max_features': [100, 110, 150, None]}]\n",
    "grid = GridSearchCV(estimator = gbm, param_grid= parameters, scoring= 'neg_mean_squared_error', cv = 10)\n",
    "grid = grid.fit(training_data, target)\n",
    "best_score = grid.best_score_\n",
    "results = grid.cv_results_\n",
    "print(best_score)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model:\n",
      " GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
      "             learning_rate=0.18, loss='ls', max_depth=3, max_features=160,\n",
      "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "             min_impurity_split=None, min_samples_leaf=1,\n",
      "             min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
      "             n_estimators=210, presort='auto', random_state=0,\n",
      "             subsample=1.0, verbose=0, warm_start=False)\n",
      "Average MSE:\n",
      " 637062847.243\n",
      "Standard deviation during CV:\n",
      " 92582215.4653\n"
     ]
    }
   ],
   "source": [
    "gbm_check = GradientBoostingRegressor(n_estimators=210, max_features=160, max_depth=3, random_state=0, \n",
    "                                       learning_rate = 0.18, min_samples_split=3)\n",
    "train_model(gbm_check, training_data, target, num_procs, mean_mse, cv_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets use Gradient Boosting with these features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(n_estimators=210, max_features=160, max_depth=3, random_state=0, \n",
    "                                       learning_rate = 0.18, min_samples_split=3)\n",
    "gbm.fit(training_data, target)\n",
    "y_pred = gbm.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of a file 'results.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(range (1461, 2920))\n",
    "with open('result.csv', 'a+') as r:\n",
    "    r.write('Id,SalePrice\\n')\n",
    "    for i in range(len(y_pred)):\n",
    "        r.write('%i,%f\\n'%(index[i], y_pred[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>120189.824695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>155980.511644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>178637.347596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>191600.348213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>173545.563516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  120189.824695\n",
       "1  1462  155980.511644\n",
       "2  1463  178637.347596\n",
       "3  1464  191600.348213\n",
       "4  1465  173545.563516"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv('result.csv')\n",
    "result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
